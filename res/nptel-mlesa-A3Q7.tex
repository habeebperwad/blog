\documentclass[12pt]{article}
\usepackage{geometry}
\geometry{
 a4paper,
 total={170mm,257mm},
 left=15mm,
 top=20mm,
 }
\usepackage{amsmath}

\usepackage[yyyymmdd]{datetime}
\usepackage{minted}
\usepackage[hidelinks]{hyperref}
\newcommand{\afmod}[1]{\text{{\color{blue} \textbf{$#1$}}}}
\newcommand{\qtext}[1]{\qquad \text{{\color{orange} \small{#1}}}}
%\newcommand{\drule}[2]{{\color{black} {if \quad}}{\color{blue} {\large \[\mathbf{#1}\]}}}
\newcommand{\drule}[2]{\[\text{If \; } {\color{blue} {\large \mathbf{#1}}} \text{, \quad then \; } {\color{blue} {\large \mathbf{#2}}}\]\\}
\begin{document}
{\Large \textbf{The Question}} \\
\rule{\textwidth}{1pt}
{\scriptsize Hints for NPTEL MLESA assignment-3/question-7, by \url{http://perwad.in} on \today\ at \currenttime} 
\vspace{20mm} \\
\noindent Consider 
\[
  J(\mathbf{w}) = \frac{1}{10} \sum_{i=1}^{5} (y^{(i)} - w_1x^{(i)} - w_0)^2 \qquad \text{where }
\mathbf{w}
= 
\begin{bmatrix}
    w_0 \\
    w_1
\end{bmatrix}
\]
\vspace{5mm} \\
\noindent and the constants $x^{(i) }$ and $y^{(i) }$ are provided in the table below: \\


\begin{tabular}{ |p{1cm}||p{2cm}|p{2cm}|  }
 \hline
 \textbf{i}& \textbf{x} &\textbf{y}\\
 \hline
 \hline
 1 & 0.00 & 0.8822 \\
 \hline
 2 & 0.25 & 1.2165 \\
 \hline
 3 & 0.50 & 1.3171 \\
 \hline
 4 & 0.75 & 1.7930 \\
 \hline
 5 & 1.00 & 1.9826 \\
\hline
\end{tabular} \\
\\

\noindent Find the $\mathbf{w}$ which minimize the J($\mathbf{w}$). 

\[
    \text{i.e.  } \operatorname*{argmin}_\mathbf{w} J(\mathbf{w})
\]



\break

{\Large \textbf{Variables in the function J}} \\
\rule{\textwidth}{1pt}
\vspace{3mm} \\
The J($\mathbf{w}$) is a multivariable function. There are only two variables in the function: $w_0$ and $w_1$.
The $x^{(i) }$ and $y^{(i) }$ are constants not variables.
Let us expand the J($\mathbf{w}$) in order to get clarified.  


\begin{eqnarray*}
J(\mathbf{w}) & = & \frac{1}{10} \sum_{i=1}^{5} (y^{(i)} - w_1x^{(i)} - w_0)^2 \\
\\
& = & \frac{1}{10} ( (y^{(1)} - x^{(1)}w_1 - w_0)^2 + 
      (y^{(2)} - x^{(2)}w_1 - w_0)^2 ) + \\ 
& &     (y^{(3)} - x^{(3)}w_1 - w_0)^2 ) + 
     (y^{(4)} - x^{(4)}w_1 - w_0)^2 ) + 
      (y^{(5)} - x^{(5)}w_1 - w_0)^2 ) 
\end{eqnarray*}

\vspace{3mm}

\noindent x and y are constants. Let us replace them with corresponding values.


\begin{eqnarray*}
J(\mathbf{w}) & = & \frac{1}{10} (\ (0.8822 -  0.00 w_1 - w_0)^2 + 
      (1.2165 - 0.25 w_1 - w_0)^2 + \\ 
& &   (1.3171 - 0.50 w_1 - w_0)^2 + 
     (1.7930 - 0.75 w_1 - w_0)^2 + 
      (1.9826 - 1.00 w_1 - w_0)^2\ ) \\
\\
& \approx & 0.5 w_0^2 + 0.5 w_0 w_1 - 1.43828 w_0 + 0.1875 w_1^2 - 0.858005 w_1 + 1.11385 
\end{eqnarray*}

\vspace{2mm}
 
It proves that there are only two variables in the function. i.e. $w_0$ and $w_1$. 
\textbf{Please cross-check} the approximation. I didn't verify the correctness of the approximation. 


\break
{\Large \textbf{Derivative Rules}} \\
\rule{\textwidth}{1pt}
\vspace{1mm} \\
\begin{itemize}
 \item \textbf{Constant Rule}
 	\drule{f(x) = c}{f'(x) = 0} 
 \item \textbf{Constant Multiple Rule}
	\drule{g(x) = c \times f(x)}{g'(x) = c \times f'(x)}
 \item \textbf{Power Rule}
	\drule{f(x) = x^n}{f'(x) = n \times x^{n-1}}
 \item \textbf{Sum and Difference Rule}
	\drule{h(x) = f(x) \pm g(x)}{h'(x) = f'(x) \pm g'(x)}
        $\text{ \qquad  \qquad Therefore,}$
	\drule{h(x) = \sum f(x)}{h'(x) = \sum f'(x) }
 \item \textbf{Product Rule}
	\drule{h(x) = f(x) \times g(x)}{h'(x) = f'(x){\times}g(x) + f(x){\times}g'(x)}
 \item \textbf{Quotient Rule}
	\drule{h(x) = \frac{f(x)}{g(x)}}{h'(x) = \frac{f'(x){\times}g(x) - f(x){\times}g'(x)}{g(x)^2}}
 \item \textbf{Chain Rule} 
	\drule{h(x) = f(g(x))}{h'(x) = f'(g(x)) \times g'(x)}
 	$\text{ \qquad  Therefore (using Power Rule too), }$
	\drule{h(x) = \big(g(x)\big)^n}{h'(x) = n \times \big(g(x)\big)^{n-1} \times g'(x) }
\end{itemize}


\break
{\Large \textbf{Gradient of the function J}} \\
\rule{\textwidth}{1pt}
\vspace{3mm} \\
\noindent Let us see $\nabla$J (gradient of J).
\[
\nabla J =
\begin{bmatrix}
    \frac{\partial J}{\partial w_0} \\
\\
    \frac{\partial J}{\partial w_1} 
\end{bmatrix}
\] \\
\vspace{5mm} \\
\noindent The partial differentiation of J with respect to $w_0$ and $w_1$:\\
\begin{eqnarray*}
%\begin{flalign*}
\frac{\partial J}{\partial w_0} & = &  \\
& = & \frac{\partial}{\partial w_0} \bigg(\ \frac{1}{10} \ \sum_{i=1}^{5} (y^{(i)} - w_1x^{(i)} - w_0)^2\ \bigg) \\
& = & \afmod{\frac{1}{10}} \ \frac{\partial}{\partial w_0} \bigg(\  \sum_{i=1}^{5} (y^{(i)} - w_1x^{(i)} - w_0)^2\ \bigg)
 \qtext{Constant Multiple Rule} \\
& = & \frac{1}{10} \ \afmod{\sum_{i=1}^{5}} \bigg(\ \frac{\partial}{\partial w_0} \big(\ y^{(i)} - w_1x^{(i)} - w_0\big)^2\ \bigg) \qtext{Sum Rule} \\
& = & \frac{1}{10} \ \sum_{i=1}^{5} \bigg(\ \afmod{2 \times \big(y^{(i)} - w_1x^{(i)} - w_0 \big)^1 \times \frac{\partial}{\partial w_0} \big(y^{(i)} - w_1x^{(i)} - w_0 \big)}\ \bigg) \qtext{Chain Rule, Power Rule} \\
& = & \frac{1}{10} \ \sum_{i=1}^{5} \bigg(\ 2 \times \big(y^{(i)}-w_1x^{(i)}-w_0 \big) \times \big(\ \afmod{\frac{\partial\ y^{(i)}}{\partial w_0} - \frac{\partial\ w_1x^{(i)}}{\partial w_0} - \frac{\partial\ w_0}{\partial w_0}} \ \big) \bigg) \qtext{Sum/Diff Rule} \\
& = & \frac{1}{10} \ \sum_{i=1}^{5} \bigg(\ 2 \times \big(y^{(i)} - w_1x^{(i)} - w_0 \big) \times \big(\ \afmod{0} - \afmod{0} - \frac{\partial \ w_0}{\partial w_0} \big) \bigg) \qtext{Constant Rule } \\
& = & \frac{1}{10} \ \sum_{i=1}^{5} \bigg(\ \afmod{-}2 \times \big( y^{(i)} - w_1x^{(i)} - w_0 \big) \times \afmod{\frac{\partial\ w_0}{\partial w_0}}\bigg)\ \qtext{Simplification} \\
& = & \frac{1}{10} \ \sum_{i=1}^{5} \bigg(\ -2 \times (y^{(i)} - w_1x^{(i)} - w_0) \times \afmod{1 \times w_0^0} \ \bigg) \qtext{Power Rule} \\
& = & \frac{1}{10} \ \sum_{i=1}^{5} \bigg(\ -2 \times (y^{(i)} - w_1x^{(i)} - w_0 \big) \ \bigg) \qtext{Simplification} \\
& = & \frac{\afmod{-2}}{10} \ \sum_{i=1}^{5} \bigg( \ y^{(i)} - w_1x^{(i)} - w_0 \ \bigg) \qtext{Constant Rule} \\
& & \quad  \sim \sim \sim \sim \sim \sim \sim \sim \sim \sim \sim \sim \sim \sim
\end{eqnarray*}
\vspace{4mm} \\
\noindent and
\vspace{3mm} \\

\begin{eqnarray*}
\frac{\partial J}{\partial w_1} & = &  \\
& = & \frac{\partial}{\partial w_1} \bigg(\ \frac{1}{10} \ \sum_{i=1}^{5} (y^{(i)} - w_1x^{(i)} - w_0)^2\ \bigg) \\
& = & \afmod{\frac{1}{10}} \ \frac{\partial}{\partial w_1} \bigg(\  \sum_{i=1}^{5} (y^{(i)} - w_1x^{(i)} - w_0)^2\ \bigg)
 \qtext{Constant Multiple Rule} \\
& = & \frac{1}{10} \ \afmod{\sum_{i=1}^{5}} \bigg(\ \frac{\partial}{\partial w_1} \big(\ y^{(i)} - w_1x^{(i)} - w_0\big)^2\ \bigg) \qtext{Sum Rule} \\
& = & \frac{1}{10} \ \sum_{i=1}^{5} \bigg(\ \afmod{2 \times \big(y^{(i)} - w_1x^{(i)} - w_0 \big)^1 \times \frac{\partial}{\partial w_1} \big(y^{(i)} - w_1x^{(i)} - w_0 \big)}\ \bigg) \qtext{Chain Rule, Power Rule} \\
& = & \frac{1}{10} \ \sum_{i=1}^{5} \bigg(\ 2 \times \big(y^{(i)}-w_1x^{(i)}-w_0 \big) \times \big(\ \afmod{\frac{\partial\ y^{(i)}}{\partial w_1} - \frac{\partial\ w_1x^{(i)}}{\partial w_1} - \frac{\partial\ w_0}{\partial w_1}} \ \big) \bigg) \qtext{Sum/Diff Rule} \\
& = & \frac{1}{10} \ \sum_{i=1}^{5} \bigg(\ 2 \times \big(y^{(i)} - w_1x^{(i)} - w_0 \big) \times \big(\ \afmod{0} - \frac{\partial\ w_1x^{(i)}}{\partial w_1} - \afmod{0} \big) \bigg) \qtext{Constant Rule } \\
& = & \frac{1}{10} \ \sum_{i=1}^{5} \bigg(\ \afmod{-}2 \times \big( y^{(i)} - w_1x^{(i)} - w_0 \big) \times \afmod{\frac{\partial\ w_1x^{(i)}}{\partial w_1}}\bigg)\ \qtext{Simplification} \\
& = & \frac{1}{10} \ \sum_{i=1}^{5} \bigg(\ \afmod{-}2 \times \big( y^{(i)} - w_1x^{(i)} - w_0 \big) \times \afmod{x^{(i)}} \times \frac{\partial\ w_1}{\partial w_1}\bigg)\ \qtext{Constant Rule} \\
& = & \frac{1}{10} \ \sum_{i=1}^{5} \bigg(\ -2 \times (y^{(i)} - w_1x^{(i)} - w_0) \times x^{(i)} \times \afmod{1 \times w_1^0} \ \bigg) \qtext{Power Rule} \\
& = & \frac{1}{10} \ \sum_{i=1}^{5} \bigg(\ -2 \times (y^{(i)} - w_1x^{(i)} - w_0 \big) \times x^{(i)} \ \bigg) \qtext{Simplification} \\
& = & \frac{\afmod{-2}}{10} \ \sum_{i=1}^{5} \bigg( \ \big( y^{(i)} - w_1x^{(i)} - w_0 \big) \times x^{(i)} \ \bigg) \qtext{Constant Rule}\\
& & \quad \sim \sim \sim \sim \sim \sim \sim \sim \sim \sim \sim \sim \sim \sim \sim \sim \sim \sim \sim \\  
\end{eqnarray*} 
 

\break
{\Large \textbf{Computing $\mathbf{w}^{new}$ from $\mathbf{w}^{old}$}} \\
\rule{\textwidth}{1pt}
\vspace{3mm} \\
\noindent Each iteration starts with a $\mathbf{w}$, say $\mathbf{w}^{old}$, and at the end of the iteration will have the updated $\mathbf{w}$, say $\mathbf{w}^{new}$.  
\[
 \mathbf{w}^{new} = \mathbf{w}^{old} - \alpha * \nabla_{\mathbf{w}^{old}} J \qquad \text{where } \alpha\ \text{is learning rate.} 
\] \\
\vspace{3mm} \\ 
\noindent Let us expand it.
\begin{eqnarray*}
\begin{bmatrix}
    w_0^{new} \\
 \\
    w_1^{new}
\end{bmatrix}
=
\begin{bmatrix}
    w_0^{old} \\
 \\
    w_1^{old}
\end{bmatrix}
-
\alpha *
\begin{bmatrix}
    \frac{\partial J}{\partial w_0^{old}} \\
\\
    \frac{\partial J}{\partial w_1^{old}} 
\end{bmatrix}  \\
\\
\\
=
\begin{bmatrix}
    w_0^{old} - \alpha * \frac{\partial J}{\partial w_0^{old}} \\
 \\
    w_1^{old} - \alpha * \frac{\partial J}{\partial w_1^{old}} 
\end{bmatrix}
\end{eqnarray*} \\
\vspace{3mm} \\ 
\noindent Have a look at the $w_0^{new}$ and $w_1^{new}$ separately. \\
\vspace{3mm}
\[
    w_0^{new} = w_0^{old} - \alpha * \frac{\partial J}{\partial w_0^{old}} \\
\]
\\
\noindent and
\\ 
\[
 w_1^{new} =  w_1^{old} - \alpha * \frac{\partial J}{\partial w_1^{old}} \\
\]


\break
{\Large \textbf{Python Code}} \\
\rule{\textwidth}{1pt}
\begin{minted}[mathescape]{python}

import numpy as np

x = np.array([0, 0.25, 0.5, 0.75, 1.00])
y = np.array([0.8822, 1.2165, 1.3171, 1.7930, 1.9826])

def J(w):
  return 1.0/10 * sum([(y[i]-w[1]*x[i]-w[0])**2 for i in range(5)])
    
def gradientJ(w):
  return np.array([
    -2.0/10 * sum([(y[i]-w[1]*x[i]-w[0]) for i in range(5)]),     # wrt w0
    -2.0/10 * sum([(y[i]-w[1]*x[i]-w[0])*x[i] for i in range(5)]) # wrt w1
  ])
  
w = np.array([0, 0])
alpha = 1.0

for k in range(1, 6):
  print("%d old W:%-24s " % (k, w), end="")
  w = w - alpha * gradientJ(w)
  print(" new W:%s " % (w))

\end{minted}

\end{document}
